const { Chroma } = require("@langchain/community/vectorstores/chroma");
const { Document } = require("@langchain/core/documents");
const { formatDocumentsAsString } = require("langchain/docstore");
const { createModels } = require("./llmFactory");

// --- Models and Store ---
let embeddings;
let vectorStore;
let llm;

// --- Configuration ---
const CHROMA_COLLECTION_NAME = "memagent-collection";

/**
 * Initializes the memory service, setting up the AI models and vector store.
 * Must be called before any other function in this module.
 */
async function init() {
  // Use the factory to get the configured models
  const models = createModels();
  llm = models.llm;
  embeddings = models.embeddings;

  // Initialize ChromaDB vector store with the correct embeddings model
  vectorStore = new Chroma(embeddings, {
    collectionName: CHROMA_COLLECTION_NAME,
  });

  console.log("Memory service initialized successfully.");
}

/**
 * Saves a user query and its corresponding AI response to the vector store.
 * @param {string} userQuery - The query sent by the user.
 * @param {string} aiResponse - The response generated by the AI.
 */
async function saveMemory(userQuery, aiResponse) {
  const doc = new Document({
    pageContent: `User Query: ${userQuery}\nAI Response: ${aiResponse}`,
    metadata: {
      timestamp: Date.now(),
      source: "chat",
    },
  });

  await vectorStore.addDocuments([doc]);
  console.log("Memory saved.");
}

/**
 * Retrieves relevant documents from memory based on a new user query.
 * @param {string} userQuery - The new query from the user.
 * @returns {Promise<string>} A formatted string of relevant context, or an empty string if no context is found.
 */
async function retrieveMemory(userQuery) {
  const retriever = vectorStore.asRetriever({ k: 2 }); // Retrieve top 2 most relevant documents
  const relevantDocs = await retriever.getRelevantDocuments(userQuery);

  if (relevantDocs.length === 0) {
    return ""; // No relevant memory found
  }

  const context = formatDocumentsAsString(relevantDocs);
  console.log("Retrieved context from memory.");
  return `\n\n--- Context from Past Interactions ---\n${context}\n--- End of Context ---`;
}

module.exports = {
  init,
  saveMemory,
  retrieveMemory,
  llm, // Exporting the llm model to be used directly in the server for chat
};
